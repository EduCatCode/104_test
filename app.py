import streamlit as st
import requests
import pandas as pd
import matplotlib.pyplot as plt
import time
import random
import re
from collections import Counter

# --- è¨­å®š Matplotlib ä¸­æ–‡å­—å‹ (é‡å°ä¸åŒä½œæ¥­ç³»çµ±) ---
import platform
system_os = platform.system()
if system_os == "Windows":
    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
elif system_os == "Darwin": # Mac
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
else:
    # Linux (Colab/Streamlit Cloud) å¯èƒ½éœ€è¦é¡å¤–å­—å‹æª”ï¼Œé€™è£¡å…ˆç”¨é è¨­é¿å…å ±éŒ¯
    pass
plt.rcParams['axes.unicode_minus'] = False

# --- æ ¸å¿ƒå‡½å¼ï¼šçˆ¬èŸ² ---
def fetch_104_jobs(keyword, pages=3):
    job_list = []
    url = "https://www.104.com.tw/jobs/search/list"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Referer": "https://www.104.com.tw/jobs/search/",
    }

    progress_bar = st.progress(0)
    status_text = st.empty()

    for page in range(1, pages + 1):
        status_text.text(f"æ­£åœ¨çˆ¬å–ç¬¬ {page}/{pages} é ...")
        params = {
            "ro": "0",
            "kwop": "7",
            "keyword": keyword,
            "expansionType": "area,spec,com,job,wf,wktm",
            "order": "1",
            "asc": "0",
            "page": page,
            "mode": "s",
            "jobsource": "2018indexpoc",
            "langFlag": "0"
        }

        try:
            response = requests.get(url, headers=headers, params=params)
            data = response.json()
            
            if "data" not in data or "list" not in data["data"] or len(data["data"]["list"]) == 0:
                break
                
            jobs = data["data"]["list"]
            
            for job in jobs:
                # è§£ææ“…é•·å·¥å…· (é€™æ˜¯ Listï¼Œéœ€è¦è½‰æˆå­—ä¸²)
                specialties = job.get("specialty", [])
                skill_str = ",".join([s['description'] for s in specialties]) if specialties else "ä¸æ‹˜"

                job_info = {
                    "è·ç¼ºåç¨±": job.get("jobName"),
                    "å…¬å¸åç¨±": job.get("custName"),
                    "åœ°å€": job.get("jobAddrNoDesc"),
                    "è–ªè³‡åŸæ–‡": job.get("salaryDesc"),
                    "å­¸æ­·": job.get("optionEdu"),
                    "ç¶“æ­·": job.get("periodDesc"),
                    "æ“…é•·å·¥å…·": skill_str, # é‡è¦ï¼šæŠ€èƒ½æ¬„ä½
                    "å·¥ä½œå…§å®¹": job.get("description", "")[:100] + "...", # å–å‰100å­—é è¦½
                    "ç¶²å€": f"https:{job.get('link').get('job')}" if job.get("link") else ""
                }
                job_list.append(job_info)
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.progress(page / pages)
            time.sleep(random.uniform(0.5, 1.5)) # ç¨å¾®å¿«ä¸€é»ï¼Œä½†ä»ä¿æŒç¦®è²Œ
            
        except Exception as e:
            st.error(f"ç¬¬ {page} é ç™¼ç”ŸéŒ¯èª¤: {e}")
            break
            
    status_text.text("çˆ¬å–å®Œæˆï¼")
    progress_bar.empty()
    return pd.DataFrame(job_list)

# --- æ ¸å¿ƒå‡½å¼ï¼šè–ªè³‡æ¸…æ´— ---
def parse_salary(salary_str):
    """
    å°‡ 'æœˆè–ª 30,000~50,000å…ƒ' è½‰æ›ç‚ºå¹³å‡å€¼ 40000
    å¿½ç•¥ 'é¢è­°', 'æ™‚è–ª' ç­‰è¤‡é›œæƒ…æ³ä»¥ç°¡åŒ–åˆ†æ
    """
    if "é¢è­°" in salary_str:
        return None
    if "æ™‚è–ª" in salary_str or "æ—¥è–ª" in salary_str:
        return None # æš«æ™‚åªåˆ†ææœˆè–ª
    
    # ç§»é™¤åƒåˆ†ä½é€—è™Ÿ
    clean_str = salary_str.replace(",", "")
    # æŠ“å–æ‰€æœ‰æ•¸å­—
    numbers = re.findall(r'\d+', clean_str)
    
    if len(numbers) == 2:
        return (int(numbers[0]) + int(numbers[1])) / 2
    elif len(numbers) == 1:
        return int(numbers[0]) # ä¾‹å¦‚ "40000å…ƒä»¥ä¸Š"
    else:
        return None

# --- Streamlit ä»‹é¢è¨­å®š ---
st.set_page_config(page_title="104 è·ç¼ºæˆ°æƒ…å®¤", page_icon="ğŸˆ", layout="wide")

# å´é‚Šæ¬„ï¼šè¨­å®šå€
st.sidebar.header("ğŸ” æœå°‹è¨­å®š")
keyword = st.sidebar.text_input("è¼¸å…¥è·ç¼ºé—œéµå­—", "Python æ•¸æ“šåˆ†æ")
pages_to_scrape = st.sidebar.slider("çˆ¬å–é æ•¸", 1, 10, 3)
run_btn = st.sidebar.button("é–‹å§‹çˆ¬å–")

st.sidebar.markdown("---")
st.sidebar.markdown("Developed by **Python Instructor**")

# ä¸»ç•«é¢
st.title("ğŸ“Š 104 äººåŠ›éŠ€è¡Œ - è·ç¼ºåˆ†ææˆ°æƒ…å®¤")
st.markdown(f"ç›®æ¨™ï¼šåˆ†æ **{keyword}** çš„å¸‚å ´éœ€æ±‚ã€è–ªè³‡åˆ†ä½ˆèˆ‡ç†±é–€æŠ€èƒ½ã€‚")

if run_btn:
    # 1. åŸ·è¡Œçˆ¬èŸ²
    df = fetch_104_jobs(keyword, pages_to_scrape)
    
    if not df.empty:
        # 2. è³‡æ–™æ¸…æ´—
        df['ç¸£å¸‚'] = df['åœ°å€'].apply(lambda x: x[:3])
        df['å¹³å‡æœˆè–ª'] = df['è–ªè³‡åŸæ–‡'].apply(parse_salary)
        
        # 3. é¡¯ç¤ºé—œéµæŒ‡æ¨™ (KPI)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æœå°‹è·ç¼ºæ•¸", f"{len(df)} ç­†")
        with col2:
            avg_salary = df['å¹³å‡æœˆè–ª'].mean()
            st.metric("å¹³å‡æœˆè–ª (é ä¼°)", f"{int(avg_salary):,} å…ƒ" if pd.notnull(avg_salary) else "ç„¡æ³•è¨ˆç®—")
        with col3:
            top_city = df['ç¸£å¸‚'].mode()[0]
            st.metric("æœ€å¤šè·ç¼ºåœ°å€", top_city)

        # 4. é ç±¤åˆ†é é¡¯ç¤º
        tab1, tab2, tab3 = st.tabs(["ğŸ“‹ è©³ç´°è³‡æ–™", "ğŸ“ˆ åœ–è¡¨åˆ†æ", "ğŸ› ï¸ æŠ€èƒ½æ–‡å­—é›²"])

        with tab1:
            st.dataframe(df, use_container_width=True)
            # ä¸‹è¼‰æŒ‰éˆ•
            csv = df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ CSV æª”æ¡ˆ",
                data=csv,
                file_name=f'104_jobs_{keyword}.csv',
                mime='text/csv',
            )

        with tab2:
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                st.subheader("ğŸ“ å„åœ°å€è·ç¼ºåˆ†ä½ˆ")
                # ä½¿ç”¨ Streamlit å…§å»ºåœ–è¡¨ï¼Œå°ä¸­æ–‡æ”¯æ´è¼ƒå¥½
                city_counts = df['ç¸£å¸‚'].value_counts()
                st.bar_chart(city_counts)

            with col_chart2:
                st.subheader("ğŸ’° è–ªè³‡åˆ†ä½ˆ (åƒ…çµ±è¨ˆæœˆè–ª)")
                if df['å¹³å‡æœˆè–ª'].notnull().sum() > 0:
                    fig, ax = plt.subplots()
                    df['å¹³å‡æœˆè–ª'].hist(bins=20, ax=ax, color='orange', edgecolor='white')
                    ax.set_title("æœˆè–ªåˆ†ä½ˆåœ–")
                    ax.set_xlabel("è–ªè³‡ (TWD)")
                    ax.set_ylabel("è·ç¼ºæ•¸")
                    st.pyplot(fig)
                else:
                    st.info("çˆ¬å–åˆ°çš„è³‡æ–™ä¸­ï¼Œå¤§éƒ¨åˆ†ç‚ºé¢è­°æˆ–ç„¡æ³•è§£æè–ªè³‡ã€‚")

        with tab3:
            st.subheader("ğŸ”¥ ä¼æ¥­æœ€è¦æ±‚çš„æŠ€èƒ½ (Top 20)")
            # çµ±è¨ˆæ‰€æœ‰æŠ€èƒ½æ¨™ç±¤
            all_skills = []
            for skills in df['æ“…é•·å·¥å…·']:
                if skills and skills != "ä¸æ‹˜":
                    all_skills.extend(skills.split(','))
            
            if all_skills:
                skill_counts = Counter(all_skills).most_common(20)
                skill_df = pd.DataFrame(skill_counts, columns=['æŠ€èƒ½', 'æ¬¡æ•¸'])
                
                # ç¹ªè£½æ°´å¹³é•·æ¢åœ–
                fig_skill, ax_skill = plt.subplots(figsize=(10, 8))
                ax_skill.barh(skill_df['æŠ€èƒ½'], skill_df['æ¬¡æ•¸'], color='lightgreen')
                ax_skill.invert_yaxis() # è®“æœ€é«˜çš„åœ¨ä¸Šé¢
                ax_skill.set_title("ç†±é–€æŠ€èƒ½çµ±è¨ˆ")
                st.pyplot(fig_skill)
            else:
                st.write("æœ¬æ¬¡æœå°‹æœªæŠ“å–åˆ°è¶³å¤ çš„æŠ€èƒ½è³‡æ–™ã€‚")

    else:
        st.warning("æ‰¾ä¸åˆ°è³‡æ–™ï¼Œè«‹å˜—è©¦æ›´æ›é—œéµå­—æˆ–æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚")

else:
    st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´è¼¸å…¥é—œéµå­—ä¸¦æŒ‰ä¸‹ã€Œé–‹å§‹çˆ¬å–ã€")